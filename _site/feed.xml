<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.9.5">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2024-07-24T00:59:43-07:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Your awesome title</title><subtitle>Write an awesome description for your new site here. You can edit this line in _config.yml. It will appear in your document head meta (for Google search results) and in your feed.xml site description.</subtitle><entry><title type="html">Summer ‚Äò24 Reads</title><link href="http://localhost:4000/2024-07-19-summerbooks2024" rel="alternate" type="text/html" title="Summer ‚Äò24 Reads" /><published>2024-07-19T00:00:00-07:00</published><updated>2024-07-19T00:00:00-07:00</updated><id>http://localhost:4000/summerbooks2024</id><content type="html" xml:base="http://localhost:4000/2024-07-19-summerbooks2024"><![CDATA[<p><img style="float: left; width: 300px; margin-right: 20px;" src="assets/images/covers/bishop_deep-learning.jpg" /></p>

<p><strong>Deep Learning: Foundations and Concepts</strong> <br />
<em>Christopher M. Bishop and Hugh Bishop</em> <br />
üê¢</p>

<p>An in-depth introduction to the various pivotal concepts, algorithms, and architectures behind the modern deep learning revolution. Quite mathematically notation dense which admittedly made some sections difficult to read, but I generally found myself leaving each chapter satisfied. Likely due to the large scope of the book, some areas are rigorously reviewed while others I felt could be expanded upon more. In particular, some topics I would have liked to have seen included more were universal approximation theorems, state space models, and deep reinforcement learning. Overall an educational textbook that I would highly recommend for learning the fundamental technical aspects behind deep learning.</p>

<p><strong>Rating:</strong> 8/10</p>]]></content><author><name>Andrew Ye</name></author><category term="other" /><summary type="html"><![CDATA[]]></summary></entry><entry><title type="html">Reflecting on NeurIPS 2023</title><link href="http://localhost:4000/2023-12-20-neurips2023" rel="alternate" type="text/html" title="Reflecting on NeurIPS 2023" /><published>2023-12-20T00:00:00-08:00</published><updated>2023-12-20T00:00:00-08:00</updated><id>http://localhost:4000/neurips2023</id><content type="html" xml:base="http://localhost:4000/2023-12-20-neurips2023"><![CDATA[<p>Last week, I attended the Thirty-seventh Conference on Neural Information Processing Systems (a.k.a. NeurIPS 2023) in New Orleans, Louisiana as part of a paper on <a href="https://openreview.net/forum?id=NxpWp0IhgB" target="_blank">structured pruning for convolutional networks</a>. Aside from the scenic waterside views, great seafood, and rich historical culture, I engaged in a deep conversation with the current landscape of artificial intelligence research and left with several key findings that I‚Äôd like to share.</p>

<p>Firstly, is that A.I. moves incredibly fast - both academically and socially. Coming in, I was under the predisposition that an academic conference would be, for lack of a better word, academic. That is, much like how classes at a university are structured, the conference would in a similar manner be slow, methodical, and incremental. However, it became immediately apparent to me that this was not the case. There was an aura of constant excitement and hurriedness that permeated the halls ‚Äì everyone was rushing <em>somewhere</em> to see <em>something</em>. And no matter what that something was, it was almost guaranteed to be fascinating. It was awe-inspiring to see the groundbreaking work done by individuals from all over the world in areas from all over the world; whether it was climate data, interpretability, education, or ethics, it seemed as if every field had passionate and intelligent minds spearheading progress within their respective communities.</p>

<p>Such a sense of progress was not lost upon observers like myself. Papers built upon and cited other papers published mere months ago. Startups and venture capitalists roamed the halls, scouting the competition while eagerly attempting to secure the latest talent. Tech demos came and went, and what was last month‚Äôs superchip became this month‚Äôs benchmark. As a friend I would meet aptly put it, ‚ÄúA.I. moves in dog years.‚Äù</p>

<p>My second discovery was that there currently exists no other field that interweaves so many disciplines as does artificial intelligence. To be an effective A.I. researcher is to be an effective scholar, programmer, writer, philosopher, mathematician, and more. The field demands and will continue to demand the best and brightest minds from every sector, and I‚Äôm excited to see the kind of innovation that will take place in the next few years. Hopefully, it‚Äôs an effort that I can meaningfully contribute to.</p>

<p>Finally, I‚Äôd like to share some papers that stood out to me. Of course, I was not able to read every poster at the conference, so this will just be a kind of non-emcompassing personal highlight reel (in no particular order):</p>

<ol>
  <li><a href="https://arxiv.org/abs/2312.05250" target="_blank">TaskMet: Task-Driven Metric Learning for Model Learning</a> <br />
<em>Dishank Bansal, Ricky T. Q. Chen, Mustafa Mukadam, Brandon Amos</em>, Meta</li>
  <li><a href="https://arxiv.org/abs/2302.08224" target="_blank">DIFUSCO: Graph-based Diffusion Solvers for Combinatorial Optimization</a> <br />
<em>Zhiqing Sun, Yiming Yang</em>, Carnegie Mellon</li>
  <li><a href="https://arxiv.org/abs/2305.16183" target="_blank">Passive learning of active causal strategies in agents and language models</a> <br />
<em>Andrew Kyle Lampinen, Stephanie C Y Chan, Ishita Dasgupta, Andrew J Nam, Jane X Wang</em>, Google Deepmind/Stanford</li>
  <li><a href="https://arxiv.org/abs/2303.11366" target="_blank">Reflexion: Language Agents with Verbal Reinforcement Learning</a> <br />
<em>Noah Shinn, Federico Cassano, Edward Berman, Ashwin Gopinath, Karthik Narasimhan, Shunyu Yao</em>, Northeastern/Princeton/MIT</li>
  <li><a href="https://arxiv.org/abs/2304.15004" target="_blank">Are Emergent Abilities of Large Language Models a Mirage?</a> <br />
<em>Rylan Schaeffer, Brando Miranda, Sanmi Koyejo</em>, Stanford</li>
  <li><a href="https://arxiv.org/abs/2305.18290" target="_blank">Direct Preference Optimization: Your Language Model is Secretly a Reward Model</a> <br />
<em>Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher D. Manning, Chelsea Finn</em>, Stanford</li>
  <li><a href="https://openreview.net/forum?id=ZJWQfgXQb6" target="_blank">The ToMCAT Dataset</a> <br />
<em>Pyarelal et al.</em>, UofA</li>
  <li><a href="https://arxiv.org/abs/2211.15060" target="_blank">Interactive Visual Feature Search</a> <br />
<em>Devon Ulrich, Ruth Fong</em>, Princeton</li>
  <li><a href="https://openreview.net/forum?id=qieeNlO3C7" target="_blank">Transformers learn through gradual rank increase</a> <br />
<em>Enric Boix-Adser√†, Etai Littwin, Emmanuel Abbe, Samy Bengio, Joshua M. Susskind</em>, Apple/MIT/EPFL</li>
  <li><a href="https://openreview.net/forum?id=Pjky9XG8zP" target="_blank">One Less Reason for Filter Pruning: Gaining Free Adversarial Robustness with Structured Grouped Kernel Pruning</a> <br />
<em>Shaochen Zhong, Zaichuan You, Jiamu Zhang, Sebastian Zhao, Zachary LeClaire, Zirui Liu, Daochen Zha, Vipin Chaudhary, Shuai Xu, Xia Hu</em>, Rice/Case Western/Berkeley</li>
  <li><a href="https://arxiv.org/abs/2307.02973" target="_blank">Pruning vs Quantization: Which is Better?</a> <br />
<em>Andrey Kuzmin, Markus Nagel, Mart van Baalen, Arash Behboodi, Tijmen Blankevoort</em>, Qualcomm</li>
  <li><a href="https://arxiv.org/abs/2306.17844" target="_blank">The Clock and the Pizza: Two Stories in Mechanistic Explanation of Neural Networks</a> <br />
<em>Ziqian Zhong, Ziming Liu, Max Tegmark, Jacob Andreas</em>, MIT</li>
</ol>

<p>Without much surprise, language models dominated this year with the paradigm set by architectures like transformers for the academic world and products like ChatGPT for the general public. However, that is not to say there was not a myriad of interesting research done in fields elsewhere. Whether or not your research is concerned with LLMs, being able to present your work and explore that of others at events like NeurIPS is quite an unparalleled experience ‚Äì and one that I would very much like to experience again soon.</p>]]></content><author><name>Andrew Ye</name></author><category term="artificial-intelligence" /><summary type="html"><![CDATA[Last week, I attended the Thirty-seventh Conference on Neural Information Processing Systems (a.k.a. NeurIPS 2023) in New Orleans, Louisiana as part of a paper on structured pruning for convolutional networks. Aside from the scenic waterside views, great seafood, and rich historical culture, I engaged in a deep conversation with the current landscape of artificial intelligence research and left with several key findings that I‚Äôd like to share.]]></summary></entry><entry><title type="html">The Purpose of This Blog</title><link href="http://localhost:4000/2023-12-14-firstpost" rel="alternate" type="text/html" title="The Purpose of This Blog" /><published>2023-12-14T00:00:00-08:00</published><updated>2023-12-14T00:00:00-08:00</updated><id>http://localhost:4000/firstpost</id><content type="html" xml:base="http://localhost:4000/2023-12-14-firstpost"><![CDATA[<p><img src="https://imgs.xkcd.com/comics/blogging.png" alt="xkcd 741" style="display:block; margin-left:auto; margin-right:auto" /></p>
<p style="text-align: center"><span style="font-weight: bold">Figure 1:</span> <span style="font-style: italic">Blogging</span>  by xkcd</p>

<p>The art of writing has spanned milennia, yet the practice of blogging ‚Äì personal subspaces for the public ‚Äì is a rather recent development. With the emergence of the internet and its ability to offer easily accessible and public personal spaces, it seemed natural for people to begin jotting their thoughts down in this environment. Such a form of quick, unbounded, and (hopefully) quality prose is one in which I recently have decided to delve into. While I generally consider myself a capable writer, I find that I tend to produce ‚Äúquality‚Äù only after several rounds of methodical deliberation. This is fine, but what I would <em>really</em> like to develop is a quick sense of what to write and how to write it. In this manner, I consider this blog as that medium to develop the skill of being able to efficiently communicate in a well-phrased and educative process.</p>

<p>As with any skill, repetition is key. I anticipate that my first few posts will read quite lackluster. However, the eventual goal is that such capabilities scale with the amount of work invested. I also certainly hope that one doesn‚Äôt misinterpret the desire for fast writing with sloppy writing. Rest assured, all of my posts will have been proofread multiple times, and I will be careful to cite relevant sources.</p>

<p>In terms of content itself, expect most of the writings here to be primarily related to my work (artificial intelligence, mathematics, computer science, and finance) and written in a fairly technical and/or philosophical manner (I will often attempt to relate the two). However, I may write every now and then about subjects like art, film, and other facets that spark my interests. Any research explorations that are too short for a full-fledged paper will also be housed here.</p>

<p>Finally, I will briefly touch on how this website was made. The site itself is stored via <a href="https://pages.github.com/" target="_blank">github pages</a>, and <a href="https://jekyllrb.com/" target="_blank">Jekyll</a> is used for static content generation. Everything else is a mixture of custom css and html, much of which was directly inspired (if not copied) from Gregory Gundersen‚Äôs <a href="https://gregorygundersen.com/" target="_blank">blog</a>, of whose minimalistic and pleasing designs I fell in love with after stumbling upon his article on <a href="https://gregorygundersen.com/blog/2020/04/11/moments/" target="_blank">moments</a>, which I highly recommend for anyone interested in probability distributions. Although I expect my website‚Äôs design to gradually change and develop its own identity, it currently stands as a near carbon copy of his, to which I am eternally thankful for the following words of his:</p>
<p style="text-align: center; font-style: italic">"Feel free to copy my website."</p>]]></content><author><name>Andrew Ye</name></author><category term="other" /><summary type="html"><![CDATA[Figure 1: Blogging by xkcd]]></summary></entry></feed>